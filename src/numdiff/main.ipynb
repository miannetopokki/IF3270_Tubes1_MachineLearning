{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.MLP import *\n",
    "from lib.Graph import *\n",
    "import random\n",
    "\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_openml('mnist_784', version=1, parser='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = mnist.data, mnist.target.astype(int)\n",
    "\n",
    "X = X.to_numpy() / 255.0  \n",
    "y = y.astype(int).to_numpy()  \n",
    "def one_hot(y, num_classes=10):\n",
    "    one_hot_encoded = np.zeros((y.shape[0], num_classes))\n",
    "    one_hot_encoded[np.arange(y.shape[0]), y] = 1\n",
    "    return one_hot_encoded\n",
    "\n",
    "y = one_hot(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y,random_state=42)\n",
    "print(f\"Dataset loaded: {X_train.shape[0]} training samples, {X_test.shape[0]} test samples.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training And Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buat model\n",
    "hidden_layer_1 = Layer(input_size=784, n_neurons=16, activation='relu', bias_init='zeros',seed=42,weight_init='he_uniform')\n",
    "hidden_layer_2 = Layer(input_size=16, n_neurons=8, activation='relu', bias_init='zeros',seed=42,weight_init='he_uniform')\n",
    "output_layer = Layer(input_size=8, n_neurons=10, activation='softmax', bias_init='zeros',seed=42,weight_init='he_uniform')\n",
    "\n",
    "layers = [hidden_layer_1, hidden_layer_2,output_layer]\n",
    "\n",
    "mlp = MLP(layers=layers, loss_function='mse',lr=0.1)\n",
    "\n",
    "# Training model\n",
    "mlp.train(X_train, y_train,X_val=X_test,y_val=y_test ,epochs=1, batch_size=1000,verbose=1)\n",
    "\n",
    "# Evaluasi di test set\n",
    "test_acc = mlp.accuracy(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nEvaluating original model...\")\n",
    "original_predictions = mlp.predict(X_test[:100])\n",
    "y_test_indices = np.argmax(y_test[:100], axis=1)\n",
    "original_accuracy = accuracy_score(y_test_indices, original_predictions)\n",
    "print(f\"Original model accuracy: {original_accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nSaving model...\")\n",
    "save_path = 'mnist_model.pkl'\n",
    "mlp.save(save_path)\n",
    "print(f\"Model saved to {save_path}\")\n",
    "\n",
    "print(\"\\nLoading model...\")\n",
    "loaded_model = MLP.load(save_path)\n",
    "\n",
    "print(\"\\nValidating loaded model...\")\n",
    "loaded_predictions = loaded_model.predict(X_test[:100])\n",
    "loaded_accuracy = accuracy_score(y_test_indices, loaded_predictions)\n",
    "print(f\"Loaded model accuracy: {loaded_accuracy:.4f}\")\n",
    "\n",
    "is_identical = np.array_equal(original_predictions, loaded_predictions)\n",
    "print(f\"\\nPredictions identical: {is_identical}\")\n",
    "\n",
    "print(\"\\nComparing model parameters:\")\n",
    "all_params_match = True\n",
    "\n",
    "if len(mlp.layers) != len(loaded_model.layers):\n",
    "    print(f\"Different number of layers: {len(mlp.layers)} vs {len(loaded_model.layers)}\")\n",
    "    all_params_match = False\n",
    "else:\n",
    "    for i, (orig_layer, loaded_layer) in enumerate(zip(mlp.layers, loaded_model.layers)):\n",
    "        weights_match = np.array_equal(orig_layer.weights, loaded_layer.weights)\n",
    "        biases_match = np.array_equal(orig_layer.biases, loaded_layer.biases)\n",
    "        \n",
    "        if weights_match and biases_match:\n",
    "            print(f\"Layer {i}: All parameters match ✓\")\n",
    "        else:\n",
    "            which_diff = []\n",
    "            if not weights_match: which_diff.append(\"weights\")\n",
    "            if not biases_match: which_diff.append(\"biases\")\n",
    "            print(f\"Layer {i}: Parameters differ ({', '.join(which_diff)}) ✗\")\n",
    "            all_params_match = False\n",
    "\n",
    "print(f\"\\nOverall parameter comparison: {'PASSED' if all_params_match else 'FAILED'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.plot_weight_distribution()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.plot_gradient_distribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Buat data random untuk testing\n",
    "np.random.seed(42)\n",
    "X_train = np.random.rand(500, 5)  # 500 sampel, 5 fitur (input layer)\n",
    "y_train = np.eye(3)[np.random.choice(3, 500)]  # 500 label one-hot, 3 kelas\n",
    "\n",
    "X_test = np.random.rand(100, 5)  # 100 sampel untuk validasi\n",
    "y_test = np.eye(3)[np.random.choice(3, 100)]  # 100 label validasi\n",
    "\n",
    "# Buat model MLP\n",
    "input_layer = Layer(input_size=5, n_neurons=8, activation='relu', bias_init='zeros', seed=42, weight_init='he_uniform')\n",
    "hidden_layer_1 = Layer(input_size=8, n_neurons=8, activation='relu', bias_init='zeros', seed=42, weight_init='he_uniform')\n",
    "hidden_layer_2 = Layer(input_size=8, n_neurons=4, activation='relu', bias_init='zeros', seed=42, weight_init='he_uniform')\n",
    "output_layer = Layer(input_size=4, n_neurons=3, activation='softmax', bias_init='zeros', seed=42, weight_init='he_uniform')\n",
    "\n",
    "layers = [input_layer, hidden_layer_1, hidden_layer_2, output_layer]\n",
    "\n",
    "mlp = MLP(layers=layers, loss_function='cce', lr=0.1)\n",
    "\n",
    "# Training model\n",
    "mlp.train(X_train, y_train, X_val=X_test, y_val=y_test, epochs=5, batch_size=50)\n",
    "\n",
    "# Evaluasi di test set\n",
    "test_acc = mlp.accuracy(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
    "\n",
    "# Plot loss\n",
    "mlp.plot_loss()\n",
    "\n",
    "# Visualisasi arsitektur model\n",
    "dot = draw_mlp(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Pengaruh Depth and Width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "width_variations = [8, 16, 32]  \n",
    "depth_variations = [1, 2, 3]  \n",
    "\n",
    "results = []\n",
    "all_mlps = {} \n",
    "\n",
    "for width in width_variations:\n",
    "    for depth in depth_variations:\n",
    "        model_name = f\"width={width}_depth={depth}\"\n",
    "        print(f\"\\n\\nTraining network with {model_name}\")\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        layers.append(Layer(input_size=784, n_neurons=width, activation='relu', \n",
    "                           bias_init='zeros', seed=42, weight_init='he_uniform'))\n",
    "        \n",
    "        for i in range(depth-1):\n",
    "            layers.append(Layer(input_size=width, n_neurons=width, activation='relu', \n",
    "                               bias_init='zeros', seed=42, weight_init='he_uniform'))\n",
    "        \n",
    "        layers.append(Layer(input_size=width, n_neurons=10, activation='softmax', \n",
    "                           bias_init='zeros', seed=42, weight_init='he_uniform'))\n",
    "        \n",
    "        mlp = MLP(layers=layers, loss_function='cce', lr=0.1)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        history = mlp.train(X_train, y_train, X_val=X_test, y_val=y_test, \n",
    "                          epochs=10, batch_size=64)\n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        all_mlps[model_name] = mlp\n",
    "        \n",
    "        test_acc = mlp.accuracy(X_test, y_test)\n",
    "        print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
    "        \n",
    "        results.append({\n",
    "            'Width': width,\n",
    "            'Depth': depth,\n",
    "            'Test Accuracy': test_acc,\n",
    "            'Training Time': training_time,\n",
    "        })\n",
    "\n",
    "        \n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n\\nResults Summary:\")\n",
    "print(results_df)\n",
    "\n",
    "best_model = results_df.loc[results_df['Test Accuracy'].idxmax()]\n",
    "print(f\"\\nBest Model Configuration:\")\n",
    "print(f\"Width: {best_model['Width']}, Depth: {best_model['Depth']}\")\n",
    "print(f\"Accuracy: {best_model['Test Accuracy']:.2f}%\")\n",
    "print(f\"Training Time: {best_model['Training Time']:.2f} seconds\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "for depth in depth_variations:\n",
    "    subset = results_df[results_df['Depth'] == depth]\n",
    "    axes[0].plot(subset['Width'], subset['Test Accuracy'], marker='o', label=f'Depth={depth}')\n",
    "    \n",
    "axes[0].set_title('Width vs Accuracy')\n",
    "axes[0].set_xlabel('Width')\n",
    "axes[0].set_ylabel('Test Accuracy (%)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "for width in width_variations:\n",
    "    subset = results_df[results_df['Width'] == width]\n",
    "    axes[1].plot(subset['Depth'], subset['Test Accuracy'], marker='o', label=f'Width={width}')\n",
    "    \n",
    "axes[1].set_title('Depth vs Accuracy')\n",
    "axes[1].set_xlabel('Depth')\n",
    "axes[1].set_ylabel('Test Accuracy (%)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "fig.suptitle('Loss Curves for All Model Configurations', fontsize=16)\n",
    "\n",
    "def plot_loss_on_axis(mlp, ax):\n",
    "    ax.plot(mlp.loss_graph, label=\"Training Loss\", color=\"red\")\n",
    "    ax.plot(mlp.valid_graph, label=\"Validation Loss\", color=\"blue\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.legend()\n",
    "\n",
    "for i, depth in enumerate(depth_variations):\n",
    "    for j, width in enumerate(width_variations):\n",
    "        model_name = f\"width={width}_depth={depth}\"\n",
    "        ax = axes[i, j]\n",
    "        plot_loss_on_axis(all_mlps[model_name], ax)\n",
    "        ax.set_title(f\"Width={width}, Depth={depth}\")\n",
    "        ax.grid(True)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Pengaruh Aktivasi Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_functions = [\"sigmoid\", \"relu\", \"tanh\", \"linear\"]\n",
    "results = {}\n",
    "\n",
    "for activation in activation_functions:\n",
    "    print(f\"aktivasi {activation}\")\n",
    "\n",
    "    hidden_layer_1 = Layer(input_size=784, n_neurons=16, activation=activation, bias_init='zeros', seed=42, weight_init='random_uniform')\n",
    "    hidden_layer_2 = Layer(input_size=16, n_neurons=8, activation=activation, bias_init='zeros', seed=42, weight_init='random_uniform')\n",
    "    output_layer = Layer(input_size=8, n_neurons=10, activation='sigmoid', bias_init='zeros', seed=42, weight_init='random_uniform')\n",
    "    layers = [hidden_layer_1, hidden_layer_2, output_layer]\n",
    "    mlp = MLP(layers=layers, loss_function='mse', lr=0.1, verbose=1)\n",
    "    mlp.train(X_train, y_train, X_val=X_test, y_val=y_test, epochs=50, batch_size=1000)\n",
    "    test_acc = mlp.accuracy(X_test, y_test)\n",
    "    print(f\"Test Accuracy ({activation}): {test_acc:.2f}%\")\n",
    "\n",
    "    results[activation] = {\n",
    "        \"loss_graph\": mlp.loss_graph,\n",
    "        \"valid_graph\": mlp.valid_graph,\n",
    "        \"weights_history\": mlp.weights_history,\n",
    "        \"gradients_history\": mlp.gradients_history\n",
    "    }\n",
    "\n",
    "\n",
    "def plot_comparison(results):\n",
    "    fig, axes = plt.subplots(4, 3, figsize=(15, 20))\n",
    "    \n",
    "    for i, (activation, res) in enumerate(results.items()):\n",
    "        ax = axes[i, 0]\n",
    "        ax.plot(res[\"loss_graph\"], label=f\"{activation}\")\n",
    "        ax.set_xlabel(\"Epochs\")\n",
    "        ax.set_ylabel(\"Training Loss\")\n",
    "        ax.set_title(f\"Training Loss - {activation}\")\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "        \n",
    "        ax = axes[i, 1]\n",
    "        weights = np.concatenate([np.ravel(w[-1]) for w in res[\"weights_history\"].values()])\n",
    "        ax.hist(weights, bins=30, alpha=0.5, density=True)\n",
    "        ax.set_xlabel(\"Weight Values\")\n",
    "        ax.set_ylabel(\"Density\")\n",
    "        ax.set_title(f\"Weight Distribution - {activation}\")\n",
    "        ax.grid(True)\n",
    "        \n",
    "        ax = axes[i, 2]\n",
    "        grads = np.concatenate([np.ravel(g[-1]) for g in res[\"gradients_history\"].values()])\n",
    "        ax.hist(grads, bins=30, alpha=0.5, density=True)\n",
    "        ax.set_xlabel(\"Gradient Values\")\n",
    "        ax.set_ylabel(\"Density\")\n",
    "        ax.set_title(f\"Gradient Distribution - {activation}\")\n",
    "        ax.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_comparison(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 Pengaruh Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = [1,0.1,0.01]\n",
    "results = {}\n",
    "\n",
    "for a in LR:\n",
    "    print(f\"LR:  {a}\")\n",
    "\n",
    "    hidden_layer_1 = Layer(input_size=784, n_neurons=16, activation='relu', bias_init='zeros', seed=42, weight_init='he_uniform')\n",
    "    hidden_layer_2 = Layer(input_size=16, n_neurons=8, activation='relu', bias_init='zeros', seed=42, weight_init='he_uniform')\n",
    "    output_layer = Layer(input_size=8, n_neurons=10, activation='softmax', bias_init='zeros', seed=42, weight_init='he_uniform')\n",
    "    layers = [hidden_layer_1, hidden_layer_2, output_layer]\n",
    "    mlp = MLP(layers=layers, loss_function='cce', lr=a, verbose=1)\n",
    "    mlp.train(X_train, y_train, X_val=X_test, y_val=y_test, epochs=50, batch_size=1000)\n",
    "    test_acc = mlp.accuracy(X_test, y_test)\n",
    "    print(f\"Test Accuracy ({a}): {test_acc:.2f}%\")\n",
    "\n",
    "    results[a] = {\n",
    "        \"loss_graph\": mlp.loss_graph,\n",
    "        \"valid_graph\": mlp.valid_graph,\n",
    "        \"weights_history\": mlp.weights_history,\n",
    "        \"gradients_history\": mlp.gradients_history\n",
    "    }\n",
    "\n",
    "\n",
    "def plot_comparison(results):\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(15, 20))\n",
    "    \n",
    "    for i, (activation, res) in enumerate(results.items()):\n",
    "        ax = axes[i, 0]\n",
    "        ax.plot(res[\"loss_graph\"], label=f\"{activation}\")\n",
    "        ax.set_xlabel(\"Epochs\")\n",
    "        ax.set_ylabel(\"Training Loss\")\n",
    "        ax.set_title(f\"Training Loss - {activation}\")\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "        \n",
    "        ax = axes[i, 1]\n",
    "        weights = np.concatenate([np.ravel(w[-1]) for w in res[\"weights_history\"].values()])\n",
    "        ax.hist(weights, bins=30, alpha=0.5, density=True)\n",
    "        ax.set_xlabel(\"Weight Values\")\n",
    "        ax.set_ylabel(\"Density\")\n",
    "        ax.set_title(f\"Weight Distribution - {activation}\")\n",
    "        ax.grid(True)\n",
    "        \n",
    "        ax = axes[i, 2]\n",
    "        grads = np.concatenate([np.ravel(g[-1]) for g in res[\"gradients_history\"].values()])\n",
    "        ax.hist(grads, bins=30, alpha=0.5, density=True)\n",
    "        ax.set_xlabel(\"Gradient Values\")\n",
    "        ax.set_ylabel(\"Density\")\n",
    "        ax.set_title(f\"Gradient Distribution - {activation}\")\n",
    "        ax.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_comparison(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4 Pengaruh Bobot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from lib.MLP import MLP, Layer  \n",
    "\n",
    "weight_init_methods = ['zeros', 'random_uniform', 'random_normal', 'xavier_uniform', 'xavier_normal', 'he_uniform', 'he_normal']\n",
    "\n",
    "results = []\n",
    "all_mlps = {}\n",
    "weight_distributions_before = {}\n",
    "weight_distributions_after = {}\n",
    "\n",
    "for init_method in weight_init_methods:\n",
    "    model_name = f\"init={init_method}\"\n",
    "    print(f\"\\n\\nTraining network with {model_name}\")\n",
    "    \n",
    "    hidden_layer_1 = Layer(input_size=784, n_neurons=16, activation='relu', \n",
    "                          bias_init='zeros', seed=42, weight_init=init_method)\n",
    "    hidden_layer_2 = Layer(input_size=16, n_neurons=8, activation='relu', \n",
    "                          bias_init='zeros', seed=42, weight_init=init_method)\n",
    "    output_layer = Layer(input_size=8, n_neurons=10, activation='softmax', \n",
    "                         bias_init='zeros', seed=42, weight_init=init_method)\n",
    "\n",
    "    layers = [hidden_layer_1, hidden_layer_2, output_layer]\n",
    "    \n",
    "    # Store initial weights\n",
    "    weight_distributions_before[model_name] = {}\n",
    "    for i, layer in enumerate(layers):\n",
    "        weight_distributions_before[model_name][f'layer_{i}'] = layer.weights.copy()\n",
    "    \n",
    "    # Create and train model\n",
    "    mlp = MLP(layers=layers, loss_function='cce', lr=0.1, verbose=1)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    mlp.train(X_train, y_train, X_val=X_test, y_val=y_test, epochs=10, batch_size=1000)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    all_mlps[model_name] = mlp\n",
    "    \n",
    "    weight_distributions_after[model_name] = {}\n",
    "    for i, layer in enumerate(layers):\n",
    "        weight_distributions_after[model_name][f'layer_{i}'] = layer.weights.copy()\n",
    "    \n",
    "    # Evaluate model\n",
    "    test_acc = mlp.accuracy(X_test, y_test)\n",
    "    print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'Initialization': init_method,\n",
    "        'Test Accuracy': test_acc,\n",
    "        'Training Time': training_time,\n",
    "    })\n",
    "    \n",
    "\n",
    "    print(f\"\\nGradient Distribution for {init_method}:\")\n",
    "    \n",
    "    mlp.plot_gradient_distribution()\n",
    "    \n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n\\nResults Summary:\")\n",
    "print(results_df)\n",
    "\n",
    "best_model = results_df.loc[results_df['Test Accuracy'].idxmax()]\n",
    "print(f\"\\nBest Model Configuration:\")\n",
    "print(f\"Initialization: {best_model['Initialization']}\")\n",
    "print(f\"Accuracy: {best_model['Test Accuracy']:.2f}%\")\n",
    "print(f\"Training Time: {best_model['Training Time']:.2f} seconds\")\n",
    "\n",
    "\n",
    "# Bobot Awal\n",
    "fig, axes = plt.subplots(3, len(weight_init_methods), figsize=(20, 12))\n",
    "fig.suptitle('Initial Weight Distributions by Layer and Initialization Method', fontsize=16)\n",
    "\n",
    "for layer_idx in range(3):  \n",
    "    for i, init_method in enumerate(weight_init_methods):\n",
    "        model_name = f\"init={init_method}\"\n",
    "        ax = axes[layer_idx, i]\n",
    "        \n",
    "        initial_weights = weight_distributions_before[model_name][f'layer_{layer_idx}'].flatten()\n",
    "        ax.hist(initial_weights, bins=50, alpha=0.7)\n",
    "        \n",
    "        if layer_idx == 0:\n",
    "            ax.set_title(init_method)\n",
    "        \n",
    "        if i == 0:\n",
    "            if layer_idx == 0:\n",
    "                ax.set_ylabel(\"Hidden Layer 1\")\n",
    "            elif layer_idx == 1:\n",
    "                ax.set_ylabel(\"Hidden Layer 2\")\n",
    "            else:\n",
    "                ax.set_ylabel(\"Output Layer\")\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()\n",
    "\n",
    "n_rows = (len(weight_init_methods) + 2) // 3\n",
    "fig, axes = plt.subplots(n_rows, 3, figsize=(18, 5*n_rows))\n",
    "fig.suptitle('Loss Curves by Initialization Method', fontsize=16)\n",
    "\n",
    "if n_rows > 1:\n",
    "    axes = axes.flatten()\n",
    "\n",
    "for i, init_method in enumerate(weight_init_methods):\n",
    "    model_name = f\"init={init_method}\"\n",
    "    \n",
    "    if n_rows == 1:\n",
    "        if i < 3:\n",
    "            ax = axes[i]\n",
    "            ax.plot(all_mlps[model_name].loss_graph, label=\"Training\", color=\"red\")\n",
    "            ax.plot(all_mlps[model_name].valid_graph, label=\"Validation\", color=\"blue\")\n",
    "            ax.set_title(f\"Init: {init_method}\")\n",
    "            ax.set_xlabel(\"Epoch\")\n",
    "            ax.set_ylabel(\"Loss\")\n",
    "            ax.legend()\n",
    "            ax.grid(True)\n",
    "    else:\n",
    "        if i < len(axes):\n",
    "            ax = axes[i]\n",
    "            ax.plot(all_mlps[model_name].loss_graph, label=\"Training\", color=\"red\")\n",
    "            ax.plot(all_mlps[model_name].valid_graph, label=\"Validation\", color=\"blue\")\n",
    "            ax.set_title(f\"Init: {init_method}\")\n",
    "            ax.set_xlabel(\"Epoch\")\n",
    "            ax.set_ylabel(\"Loss\")\n",
    "            ax.legend()\n",
    "            ax.grid(True)\n",
    "\n",
    "if n_rows > 1:\n",
    "    for j in range(i+1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
