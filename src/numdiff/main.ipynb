{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.MLP import *\n",
    "from lib.Graph import *\n",
    "import random\n",
    "\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_openml('mnist_784', version=1, parser='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = mnist.data, mnist.target.astype(int)\n",
    "\n",
    "X = X.to_numpy() / 255.0  \n",
    "y = y.astype(int).to_numpy()  \n",
    "def one_hot(y, num_classes=10):\n",
    "    one_hot_encoded = np.zeros((y.shape[0], num_classes))\n",
    "    one_hot_encoded[np.arange(y.shape[0]), y] = 1\n",
    "    return one_hot_encoded\n",
    "\n",
    "y = one_hot(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=10000, stratify=y,random_state=42)\n",
    "num_data = 60000\n",
    "\n",
    "# Split data (60,000 train, 10,000 test)\n",
    "X_train, X_test = X[:num_data], X[num_data:]\n",
    "y_train, y_test = y[:num_data], y[num_data:]\n",
    "print(f\"Dataset loaded: {X_train.shape[0]} training samples, {X_test.shape[0]} test samples.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buat model\n",
    "input_layer = Layer(input_size=784, n_neurons=8, activation='relu', bias_init='zeros',seed=42,weight_init='he_uniform')\n",
    "hidden_layer_1 = Layer(input_size=8, n_neurons=8, activation='relu', bias_init='zeros',seed=42,weight_init='he_uniform')\n",
    "hidden_layer_2 = Layer(input_size=8, n_neurons=4, activation='relu', bias_init='zeros',seed=42,weight_init='he_uniform')\n",
    "output_layer = Layer(input_size=4, n_neurons=10, activation='softmax', bias_init='zeros',seed=42,weight_init='he_uniform')\n",
    "\n",
    "layers = [input_layer, hidden_layer_1, hidden_layer_2,output_layer]\n",
    "\n",
    "mlp = MLP(layers=layers, loss_function='cce',lr=0.1,verbose=1)\n",
    "\n",
    "# Training model\n",
    "mlp.train(X_train, y_train,X_val=X_test,y_val=y_test ,epochs=5, batch_size=100)\n",
    "\n",
    "# Evaluasi di test set\n",
    "test_acc = mlp.accuracy(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nEvaluating original model...\")\n",
    "original_predictions = mlp.predict(X_test[:100])\n",
    "y_test_indices = np.argmax(y_test[:100], axis=1)\n",
    "original_accuracy = accuracy_score(y_test_indices, original_predictions)\n",
    "print(f\"Original model accuracy: {original_accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nSaving model...\")\n",
    "save_path = 'mnist_model.pkl'\n",
    "mlp.save(save_path)\n",
    "print(f\"Model saved to {save_path}\")\n",
    "\n",
    "print(\"\\nLoading model...\")\n",
    "loaded_model = MLP.load(save_path)\n",
    "\n",
    "print(\"\\nValidating loaded model...\")\n",
    "loaded_predictions = loaded_model.predict(X_test[:100])\n",
    "loaded_accuracy = accuracy_score(y_test_indices, loaded_predictions)\n",
    "print(f\"Loaded model accuracy: {loaded_accuracy:.4f}\")\n",
    "\n",
    "is_identical = np.array_equal(original_predictions, loaded_predictions)\n",
    "print(f\"\\nPredictions identical: {is_identical}\")\n",
    "\n",
    "print(\"\\nComparing model parameters:\")\n",
    "all_params_match = True\n",
    "\n",
    "if len(mlp.layers) != len(loaded_model.layers):\n",
    "    print(f\"Different number of layers: {len(mlp.layers)} vs {len(loaded_model.layers)}\")\n",
    "    all_params_match = False\n",
    "else:\n",
    "    for i, (orig_layer, loaded_layer) in enumerate(zip(mlp.layers, loaded_model.layers)):\n",
    "        weights_match = np.array_equal(orig_layer.weights, loaded_layer.weights)\n",
    "        biases_match = np.array_equal(orig_layer.biases, loaded_layer.biases)\n",
    "        \n",
    "        if weights_match and biases_match:\n",
    "            print(f\"Layer {i}: All parameters match ✓\")\n",
    "        else:\n",
    "            which_diff = []\n",
    "            if not weights_match: which_diff.append(\"weights\")\n",
    "            if not biases_match: which_diff.append(\"biases\")\n",
    "            print(f\"Layer {i}: Parameters differ ({', '.join(which_diff)}) ✗\")\n",
    "            all_params_match = False\n",
    "\n",
    "print(f\"\\nOverall parameter comparison: {'PASSED' if all_params_match else 'FAILED'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.plot_weight_distribution()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.plot_gradient_distribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing graph skala kecil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Buat data random untuk testing\n",
    "np.random.seed(42)\n",
    "X_train = np.random.rand(500, 5)  # 500 sampel, 5 fitur (input layer)\n",
    "y_train = np.eye(3)[np.random.choice(3, 500)]  # 500 label one-hot, 3 kelas\n",
    "\n",
    "X_test = np.random.rand(100, 5)  # 100 sampel untuk validasi\n",
    "y_test = np.eye(3)[np.random.choice(3, 100)]  # 100 label validasi\n",
    "\n",
    "# Buat model MLP\n",
    "input_layer = Layer(input_size=5, n_neurons=8, activation='relu', bias_init='zeros', seed=42, weight_init='he_uniform')\n",
    "hidden_layer_1 = Layer(input_size=8, n_neurons=8, activation='relu', bias_init='zeros', seed=42, weight_init='he_uniform')\n",
    "hidden_layer_2 = Layer(input_size=8, n_neurons=4, activation='relu', bias_init='zeros', seed=42, weight_init='he_uniform')\n",
    "output_layer = Layer(input_size=4, n_neurons=3, activation='softmax', bias_init='zeros', seed=42, weight_init='he_uniform')\n",
    "\n",
    "layers = [input_layer, hidden_layer_1, hidden_layer_2, output_layer]\n",
    "\n",
    "mlp = MLP(layers=layers, loss_function='cce', lr=0.1)\n",
    "\n",
    "# Training model\n",
    "mlp.train(X_train, y_train, X_val=X_test, y_val=y_test, epochs=5, batch_size=50)\n",
    "\n",
    "# Evaluasi di test set\n",
    "test_acc = mlp.accuracy(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
    "\n",
    "# Plot loss\n",
    "mlp.plot_loss()\n",
    "\n",
    "# Visualisasi arsitektur model\n",
    "dot = draw_mlp(mlp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
